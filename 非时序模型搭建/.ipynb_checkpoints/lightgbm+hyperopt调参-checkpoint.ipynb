{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a23abb26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T03:29:56.621605Z",
     "start_time": "2024-04-25T03:29:55.237793Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "特征组合：Dict+GroupBy+nlp\n",
    "特征选择方式：Wrapper\n",
    "参数寻优办法：hyperopt\n",
    "模型：lightgbm\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from hyperopt import hp, fmin, tpe \n",
    "from numpy.random import RandomState\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "path = '/Volumes/U397/机器学习/机器学习比赛/机器学习算法竞赛实战/第8章 kaggle 信用卡忠诚度预测/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103d3729",
   "metadata": {},
   "source": [
    "# lgb+hyperopt 线下CV得分3.6679(相比baseline-0.0178）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1a68751",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T12:52:51.176231Z",
     "start_time": "2024-04-25T12:52:51.153892Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_data(debug=True):\n",
    "    \"\"\"\n",
    "    读取数据\n",
    "    :param debug:是否调试版，可以极大节省debug时间\n",
    "    :return:训练集，测试集\n",
    "    \"\"\"\n",
    "    print(\"read_data...\")\n",
    "    NROWS = 10000 if debug else None\n",
    "    train_dict = pd.read_csv(path+\"preprocess/train_dict.csv\", nrows=NROWS) #nrows=None是读取全部行\n",
    "    test_dict = pd.read_csv(path+\"preprocess/test_dict.csv\", nrows=NROWS)\n",
    "    train_groupby = pd.read_csv(path+\"preprocess/train_groupby.csv\", nrows=NROWS)\n",
    "    test_groupby = pd.read_csv(path+\"preprocess/test_groupby.csv\", nrows=NROWS)\n",
    "\n",
    "    # 去除重复列\n",
    "    for co in train_dict.columns:\n",
    "        if co in train_groupby.columns and co!='card_id':\n",
    "            del train_groupby[co]\n",
    "    for co in test_dict.columns:\n",
    "        if co in test_groupby.columns and co!='card_id':\n",
    "            del test_groupby[co]\n",
    "\n",
    "    train = pd.merge(train_dict, train_groupby, how='left', on='card_id')\n",
    "    test = pd.merge(test_dict, test_groupby, how='left', on='card_id')\n",
    "    print(\"done\")\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def feature_select_wrapper(train, test):\n",
    "    \"\"\"\n",
    "    按照lightgbm的feature_importance进行特征筛选\n",
    "    :param train:\n",
    "    :param test:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    print('feature_select_wrapper...')\n",
    "    label = 'target'\n",
    "    features = train.columns.tolist()\n",
    "    features.remove('card_id')\n",
    "    features.remove('target')\n",
    "\n",
    "    # 配置模型的训练参数\n",
    "    params_initial = {\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.1,\n",
    "        'boosting': 'gbdt',\n",
    "        'min_child_samples': 20,\n",
    "        'bagging_seed': 2020,\n",
    "        'bagging_fraction': 0.7, #行采样\n",
    "        'bagging_freq': 1,#重新采样的频率，每k次迭代后重新按照bagging_fraction比例进行行采样\n",
    "        'feature_fraction': 0.7, #列采样\n",
    "        'max_depth': -1,\n",
    "        'metric': 'rmse',\n",
    "        'reg_alpha': 0,\n",
    "        'reg_lambda': 1,\n",
    "        'objective': 'regression'\n",
    "    }\n",
    "    ESR = 30    #early_stop_round\n",
    "    NBR = 10000 #num_boost_round\n",
    "    VBE = 50    #verbose_eval\n",
    "    callbacks = [lgb.log_evaluation(VBE),lgb.early_stopping(ESR)]\n",
    "    \n",
    "    kf = KFold(n_splits=5, random_state=2020, shuffle=True)\n",
    "    fse = pd.Series(0, index=features)\n",
    "    for i,(train_part_index, val_index) in enumerate(kf.split(train[features], train[label])):\n",
    "        # 模型训练\n",
    "        train_part = lgb.Dataset(train[features].loc[train_part_index],\n",
    "                                 train[label].loc[train_part_index])\n",
    "        val_part = lgb.Dataset(train[features].loc[val_index],\n",
    "                           train[label].loc[val_index])\n",
    "        bst = lgb.train(params_initial, train_part, num_boost_round=NBR,\n",
    "                        valid_sets=[train_part, val_part],\n",
    "                        valid_names=['train', 'valid'],\n",
    "                        callbacks=callbacks)\n",
    "        '''\n",
    "        early_stopping_rounds : int or None, optional (default=None)\n",
    "            Activates early stopping. The model will train until the validation score stops improving.\n",
    "        Validation score needs to improve at least every ``early_stopping_rounds`` round(s)\n",
    "        to continue training.\n",
    "            Requires at least one validation data and one metric.\n",
    "        注意If there's more than one, will check all of them. But the training data is ignored anyway.\n",
    "        To check only the first metric, set the ``first_metric_only`` parameter to ``True`` in ``params``.\n",
    "        The index of iteration that has the best performance will be saved in the ``best_iteration`` field\n",
    "        if early stopping logic is enabled by setting ``early_stopping_rounds``\n",
    "        '''\n",
    "        fse += pd.Series(bst.feature_importance(), features) #统计5折加总的feature_importance\n",
    "        #print(pd.Series(bst.feature_importance(), features).head(10))\n",
    "        #print(fse.head(10))\n",
    "\n",
    "    feature_select = ['card_id'] + fse.sort_values(ascending=False).index.tolist()[:300]\n",
    "    print('done')\n",
    "    return train[feature_select + ['target']], test[feature_select]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "add17830",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T10:00:35.939086Z",
     "start_time": "2024-04-25T10:00:35.919033Z"
    }
   },
   "outputs": [],
   "source": [
    "def params_append(params):\n",
    "    \"\"\"\n",
    "    默认情况下，在构造LightGBM数据集对象时，将基于min_data_in_leaf的值过滤掉某些特征。\n",
    "    举一个简单的例子，考虑一个具有一个名为feature_1的特征的1000个观测数据集。 \n",
    "    feature_1仅采用两个值：25.0（995个观测值）和50.0（5个观测值）。如果min_data_in_leaf = 10,则此特征没有拆分。\n",
    "    在构建数据集之前，LightGBM不会在重新训练时重新考虑此特征并每次迭代时都忽略它，而是在训练之前将其过滤掉。\n",
    "    可以通过设置feature_pre_filter = False来覆盖此默认行为，以免在hyperopt调参报错\n",
    "    \"\"\"\n",
    "    params['feature_pre_filter'] = False\n",
    "    params['objective'] = 'regression'\n",
    "    params['metric'] = 'rmse'\n",
    "    params['bagging_seed'] = 2020\n",
    "    return params\n",
    "\n",
    "\n",
    "def param_hyperopt(train):\n",
    "    \"\"\"\n",
    "    返回最佳参数\n",
    "    :param train:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    label = 'target'\n",
    "    features = train.columns.tolist()\n",
    "    features.remove('card_id')\n",
    "    features.remove('target')\n",
    "    train_data = lgb.Dataset(train[features], train[label], silent=True)\n",
    "    def hyperopt_objective(params):#优化的目标函数\n",
    "        \"\"\"\n",
    "        :param params:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        params = params_append(params)\n",
    "        print(params)\n",
    "        callbacks = [lgb.early_stopping(20)]\n",
    "        res = lgb.cv(params, train_data, 1000,\n",
    "                     nfold=2,\n",
    "                     stratified=False,\n",
    "                     shuffle=True,\n",
    "                     metrics='rmse',\n",
    "                     callbacks=callbacks,\n",
    "                     show_stdv=False,\n",
    "                     seed=2020)\n",
    "        #result记录了每一颗树的eval_metric,因此rmse-mean中的最小值即对应最后的最优结果\n",
    "        return min(res['rmse-mean'])#目标函数 可以自己选择 这里为什么是min？\n",
    "    \n",
    "    params_space = {\n",
    "        'learning_rate': hp.uniform('learning_rate', 1e-2, 5e-1), #均匀分布\n",
    "        'bagging_fraction': hp.uniform('bagging_fraction', 0.5, 1),\n",
    "        'feature_fraction': hp.uniform('feature_fraction', 0.5, 1),\n",
    "        'num_leaves': hp.choice('num_leaves', list(range(10, 300, 10))),\n",
    "        'reg_alpha': hp.randint('reg_alpha', 0, 10),#随机整数\n",
    "        'reg_lambda': hp.uniform('reg_lambda', 0, 10),\n",
    "        'bagging_freq': hp.randint('bagging_freq', 1, 10),\n",
    "        'min_child_samples': hp.choice('min_child_samples', list(range(1, 30, 5)))\n",
    "        }\n",
    "    #fmin:Minimize a function over a hyperparameter space.\n",
    "    params_best = fmin(\n",
    "        hyperopt_objective,\n",
    "        space=params_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=30,\n",
    "        rstate=np.random.default_rng(2020))\n",
    "    return params_best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b49e8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T03:23:16.689701Z",
     "start_time": "2024-04-25T03:23:16.683261Z"
    }
   },
   "outputs": [],
   "source": [
    "tpe.suggest?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7413f14c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T10:11:38.726721Z",
     "start_time": "2024-04-25T10:00:41.407899Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_data...\n",
      "done\n",
      "feature_select_wrapper...\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.233500 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 227965\n",
      "[LightGBM] [Info] Number of data points in the train set: 161533, number of used features: 1637\n",
      "[LightGBM] [Info] Start training from score -0.390986\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\ttrain's rmse: 3.43102\tvalid's rmse: 3.70777\n",
      "Early stopping, best iteration is:\n",
      "[57]\ttrain's rmse: 3.41246\tvalid's rmse: 3.70645\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.286932 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 228067\n",
      "[LightGBM] [Info] Number of data points in the train set: 161533, number of used features: 1640\n",
      "[LightGBM] [Info] Start training from score -0.396781\n",
      "[50]\ttrain's rmse: 3.44484\tvalid's rmse: 3.66146\n",
      "Early stopping, best iteration is:\n",
      "[66]\ttrain's rmse: 3.39923\tvalid's rmse: 3.65861\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.290840 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 228163\n",
      "[LightGBM] [Info] Number of data points in the train set: 161534, number of used features: 1644\n",
      "[LightGBM] [Info] Start training from score -0.390348\n",
      "[50]\ttrain's rmse: 3.42185\tvalid's rmse: 3.73025\n",
      "Early stopping, best iteration is:\n",
      "[66]\ttrain's rmse: 3.39923\tvalid's rmse: 3.65861\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.241420 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 228023\n",
      "[LightGBM] [Info] Number of data points in the train set: 161534, number of used features: 1639\n",
      "[LightGBM] [Info] Start training from score -0.391392\n",
      "[50]\ttrain's rmse: 3.4108\tvalid's rmse: 3.78929\n",
      "Early stopping, best iteration is:\n",
      "[66]\ttrain's rmse: 3.39923\tvalid's rmse: 3.65861\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.227186 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 227966\n",
      "[LightGBM] [Info] Number of data points in the train set: 161534, number of used features: 1637\n",
      "[LightGBM] [Info] Start training from score -0.398675\n",
      "[50]\ttrain's rmse: 3.46176\tvalid's rmse: 3.58679\n",
      "Early stopping, best iteration is:\n",
      "[50]\ttrain's rmse: 3.46176\tvalid's rmse: 3.58679\n",
      "done\n",
      "{'bagging_fraction': 0.9429104308567877, 'bagging_freq': 2, 'feature_fraction': 0.5715782198140802, 'learning_rate': 0.21315219327595428, 'min_child_samples': 30, 'num_leaves': 160, 'reg_alpha': 3, 'reg_lambda': 7.561160634893758, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "  0%|                                    | 0/30 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ziyan/opt/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py:1491: UserWarning: 'silent' argument is deprecated and will be removed in a future release of LightGBM. Pass 'verbose' parameter via 'params' instead.\n",
      "  _log_warning(\"'silent' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022806 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017739 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "Training until validation scores don't improve for 20 rounds                    \n",
      "Early stopping, best iteration is:                                              \n",
      "[8]\tcv_agg's rmse: 3.73163 + 0.00627336\n",
      "{'bagging_fraction': 0.512262561313273, 'bagging_freq': 3, 'feature_fraction': 0.6864235320941958, 'learning_rate': 0.31527024380943564, 'min_child_samples': 21, 'num_leaves': 180, 'reg_alpha': 2, 'reg_lambda': 7.569488156706784, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057096 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060428 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "Training until validation scores don't improve for 20 rounds                    \n",
      "Early stopping, best iteration is:                                              \n",
      "[3]\tcv_agg's rmse: 3.77412 + 0.00912465\n",
      "{'bagging_fraction': 0.8893476235428235, 'bagging_freq': 7, 'feature_fraction': 0.7135664981551813, 'learning_rate': 0.2903641143253666, 'min_child_samples': 45, 'num_leaves': 210, 'reg_alpha': 8, 'reg_lambda': 5.393157199547058, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026282 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023675 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "Training until validation scores don't improve for 20 rounds                    \n",
      "Early stopping, best iteration is:                                              \n",
      "[4]\tcv_agg's rmse: 3.74469 + 0.00588027\n",
      "{'bagging_fraction': 0.6787796182704178, 'bagging_freq': 1, 'feature_fraction': 0.7013446622253534, 'learning_rate': 0.4739191471915275, 'min_child_samples': 45, 'num_leaves': 190, 'reg_alpha': 0, 'reg_lambda': 7.368279836937887, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059121 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059596 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "Training until validation scores don't improve for 20 rounds                    \n",
      "Early stopping, best iteration is:                                              \n",
      "[2]\tcv_agg's rmse: 3.77138 + 0.00284489\n",
      "{'bagging_fraction': 0.7626783188810151, 'bagging_freq': 6, 'feature_fraction': 0.8391402503316614, 'learning_rate': 0.36741303605044506, 'min_child_samples': 48, 'num_leaves': 40, 'reg_alpha': 4, 'reg_lambda': 9.629414851907006, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027444 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "Training until validation scores don't improve for 20 rounds                    \n",
      "Early stopping, best iteration is:                                              \n",
      "[5]\tcv_agg's rmse: 3.73161 + 0.010001\n",
      "{'bagging_fraction': 0.7897880317843915, 'bagging_freq': 1, 'feature_fraction': 0.6864530344942663, 'learning_rate': 0.16801009574165351, 'min_child_samples': 27, 'num_leaves': 160, 'reg_alpha': 6, 'reg_lambda': 7.928417982876395, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060091 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059212 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "Training until validation scores don't improve for 20 rounds                    \n",
      "Early stopping, best iteration is:                                              \n",
      "[11]\tcv_agg's rmse: 3.72853 + 0.00731655\n",
      "{'bagging_fraction': 0.5709455293113112, 'bagging_freq': 6, 'feature_fraction': 0.6817716125366499, 'learning_rate': 0.28149790303677774, 'min_child_samples': 48, 'num_leaves': 50, 'reg_alpha': 2, 'reg_lambda': 0.9202721245005685, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063334 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066290 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "Training until validation scores don't improve for 20 rounds                    \n",
      "Early stopping, best iteration is:                                              \n",
      "[7]\tcv_agg's rmse: 3.74294 + 0.00266639\n",
      "{'bagging_fraction': 0.8894268166893926, 'bagging_freq': 7, 'feature_fraction': 0.8614864228571265, 'learning_rate': 0.10838878622896095, 'min_child_samples': 36, 'num_leaves': 70, 'reg_alpha': 7, 'reg_lambda': 4.128216047488188, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063052 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027382 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "Training until validation scores don't improve for 20 rounds                    \n",
      "Early stopping, best iteration is:                                              \n",
      "[34]\tcv_agg's rmse: 3.70949 + 0.0037818\n",
      "{'bagging_fraction': 0.8689240389616162, 'bagging_freq': 6, 'feature_fraction': 0.8839374526657198, 'learning_rate': 0.1295409886684552, 'min_child_samples': 24, 'num_leaves': 120, 'reg_alpha': 1, 'reg_lambda': 0.8934485689740768, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028223 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026607 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "Training until validation scores don't improve for 20 rounds                    \n",
      "Early stopping, best iteration is:                                              \n",
      "[14]\tcv_agg's rmse: 3.72757 + 0.0114862\n",
      "{'bagging_fraction': 0.9827696256425138, 'bagging_freq': 6, 'feature_fraction': 0.6241629598983252, 'learning_rate': 0.2693431830624993, 'min_child_samples': 36, 'num_leaves': 30, 'reg_alpha': 7, 'reg_lambda': 5.653589408911715, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062016 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065490 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "Training until validation scores don't improve for 20 rounds                    \n",
      "Early stopping, best iteration is:                                              \n",
      "[17]\tcv_agg's rmse: 3.71553 + 0.00549615\n",
      "{'bagging_fraction': 0.5890956497689592, 'bagging_freq': 8, 'feature_fraction': 0.9337630517879247, 'learning_rate': 0.1227160059413021, 'min_child_samples': 27, 'num_leaves': 220, 'reg_alpha': 2, 'reg_lambda': 0.31991676687450954, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026659 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068721 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "Training until validation scores don't improve for 20 rounds                    \n",
      "Early stopping, best iteration is:                                              \n",
      "[12]\tcv_agg's rmse: 3.7322 + 0.00964053\n",
      "{'bagging_fraction': 0.8729748815879685, 'bagging_freq': 2, 'feature_fraction': 0.6729872738023669, 'learning_rate': 0.28953159858971855, 'min_child_samples': 24, 'num_leaves': 250, 'reg_alpha': 9, 'reg_lambda': 4.085962466064952, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064620 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062483 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "Training until validation scores don't improve for 20 rounds                    \n",
      "Early stopping, best iteration is:                                              \n",
      "[4]\tcv_agg's rmse: 3.75821 + 0.00557323\n",
      "{'bagging_fraction': 0.7309889558300577, 'bagging_freq': 5, 'feature_fraction': 0.8837040135439155, 'learning_rate': 0.31983542597602754, 'min_child_samples': 51, 'num_leaves': 260, 'reg_alpha': 8, 'reg_lambda': 6.723833935558674, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031130 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029783 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "Training until validation scores don't improve for 20 rounds                    \n",
      "Early stopping, best iteration is:                                              \n",
      "[3]\tcv_agg's rmse: 3.74679 + 0.0103702\n",
      "{'bagging_fraction': 0.9667073801956272, 'bagging_freq': 7, 'feature_fraction': 0.8411933464820867, 'learning_rate': 0.10280030920385813, 'min_child_samples': 33, 'num_leaves': 270, 'reg_alpha': 9, 'reg_lambda': 3.059522549630802, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066679 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030084 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "Training until validation scores don't improve for 20 rounds                    \n",
      "Early stopping, best iteration is:                                              \n",
      "[23]\tcv_agg's rmse: 3.72112 + 0.00931603\n",
      "{'bagging_fraction': 0.6843213475295423, 'bagging_freq': 4, 'feature_fraction': 0.7924623289134336, 'learning_rate': 0.3020635372075119, 'min_child_samples': 36, 'num_leaves': 120, 'reg_alpha': 6, 'reg_lambda': 2.0996363046714714, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067841 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026370 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "Training until validation scores don't improve for 20 rounds                    \n",
      "Early stopping, best iteration is:                                              \n",
      "[4]\tcv_agg's rmse: 3.75514 + 0.00964484\n",
      "{'bagging_fraction': 0.7318956453136132, 'bagging_freq': 9, 'feature_fraction': 0.5776201196248023, 'learning_rate': 0.3717731127680832, 'min_child_samples': 30, 'num_leaves': 80, 'reg_alpha': 3, 'reg_lambda': 4.470202857823412, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018625 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021778 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "Training until validation scores don't improve for 20 rounds                    \n",
      "Early stopping, best iteration is:                                              \n",
      "[3]\tcv_agg's rmse: 3.7579 + 0.00877285\n",
      "{'bagging_fraction': 0.7641897846788224, 'bagging_freq': 3, 'feature_fraction': 0.6509590691041207, 'learning_rate': 0.3065257082129887, 'min_child_samples': 51, 'num_leaves': 160, 'reg_alpha': 5, 'reg_lambda': 1.776182232691963, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067075 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064953 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "Training until validation scores don't improve for 20 rounds                    \n",
      "Early stopping, best iteration is:                                              \n",
      "[4]\tcv_agg's rmse: 3.74187 + 0.010981\n",
      "{'bagging_fraction': 0.9370719655323888, 'bagging_freq': 4, 'feature_fraction': 0.6972885246773278, 'learning_rate': 0.4259177413670385, 'min_child_samples': 42, 'num_leaves': 100, 'reg_alpha': 9, 'reg_lambda': 9.310430097750862, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064519 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031477 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "Training until validation scores don't improve for 20 rounds                    \n",
      "Early stopping, best iteration is:                                              \n",
      "[3]\tcv_agg's rmse: 3.74475 + 0.00177386\n",
      "{'bagging_fraction': 0.9699889069149303, 'bagging_freq': 3, 'feature_fraction': 0.8494538930974609, 'learning_rate': 0.3302708934751551, 'min_child_samples': 42, 'num_leaves': 130, 'reg_alpha': 9, 'reg_lambda': 3.9562017043556583, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029794 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030531 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "Training until validation scores don't improve for 20 rounds                    \n",
      "Early stopping, best iteration is:                                              \n",
      "[4]\tcv_agg's rmse: 3.7481 + 0.00881496\n",
      "{'bagging_fraction': 0.8133108300550449, 'bagging_freq': 8, 'feature_fraction': 0.539933748479144, 'learning_rate': 0.04470502280634237, 'min_child_samples': 33, 'num_leaves': 290, 'reg_alpha': 5, 'reg_lambda': 0.1666976761518424, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021968 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018726 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "Training until validation scores don't improve for 20 rounds                    \n",
      "Early stopping, best iteration is:                                              \n",
      "[59]\tcv_agg's rmse: 3.69565 + 0.00691749\n",
      "{'bagging_fraction': 0.8198879482271282, 'bagging_freq': 8, 'feature_fraction': 0.9662116573408623, 'learning_rate': 0.029056635013937043, 'min_child_samples': 33, 'num_leaves': 70, 'reg_alpha': 7, 'reg_lambda': 2.7194579245643924, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029108 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033697 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "Training until validation scores don't improve for 20 rounds                    \n",
      "Early stopping, best iteration is:                                              \n",
      "[181]\tcv_agg's rmse: 3.68983 + 0.00537155\n",
      "{'bagging_fraction': 0.8222610719748087, 'bagging_freq': 8, 'feature_fraction': 0.501231962870805, 'learning_rate': 0.014424223898075149, 'min_child_samples': 33, 'num_leaves': 290, 'reg_alpha': 5, 'reg_lambda': 2.605868414180259, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021011 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022837 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "Training until validation scores don't improve for 20 rounds                    \n",
      "Early stopping, best iteration is:                                              \n",
      "[221]\tcv_agg's rmse: 3.68478 + 0.00838094\n",
      "{'bagging_fraction': 0.8193590157152683, 'bagging_freq': 8, 'feature_fraction': 0.9787470242399412, 'learning_rate': 0.015997928806422902, 'min_child_samples': 33, 'num_leaves': 70, 'reg_alpha': 7, 'reg_lambda': 2.7353714174955788, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028689 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067274 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "Training until validation scores don't improve for 20 rounds                    \n",
      "Early stopping, best iteration is:                                              \n",
      "[323]\tcv_agg's rmse: 3.69107 + 0.00616052\n",
      "{'bagging_fraction': 0.6783199840685112, 'bagging_freq': 8, 'feature_fraction': 0.9964113127503943, 'learning_rate': 0.05095112873985771, 'min_child_samples': 39, 'num_leaves': 200, 'reg_alpha': 5, 'reg_lambda': 2.968114088885729, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029974 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029178 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "Training until validation scores don't improve for 20 rounds                    \n",
      "Early stopping, best iteration is:                                              \n",
      "[40]\tcv_agg's rmse: 3.70824 + 0.0074423\n",
      "{'bagging_fraction': 0.8358605361366308, 'bagging_freq': 8, 'feature_fraction': 0.7793066934001441, 'learning_rate': 0.013323489235174044, 'min_child_samples': 33, 'num_leaves': 280, 'reg_alpha': 0, 'reg_lambda': 1.7020790316471022, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027578 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066083 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "Training until validation scores don't improve for 20 rounds                    \n",
      "Early stopping, best iteration is:                                              \n",
      "[220]\tcv_agg's rmse: 3.69066 + 0.00751533\n",
      "{'bagging_fraction': 0.6403532142891171, 'bagging_freq': 8, 'feature_fraction': 0.5034294714389296, 'learning_rate': 0.21444597482136937, 'min_child_samples': 33, 'num_leaves': 290, 'reg_alpha': 7, 'reg_lambda': 6.0748805441877165, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019792 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017965 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "Training until validation scores don't improve for 20 rounds                    \n",
      "Early stopping, best iteration is:                                              \n",
      "[8]\tcv_agg's rmse: 3.73846 + 0.00295492\n",
      "{'bagging_fraction': 0.907813520674998, 'bagging_freq': 5, 'feature_fraction': 0.9451268493017603, 'learning_rate': 0.07796335300461185, 'min_child_samples': 33, 'num_leaves': 10, 'reg_alpha': 4, 'reg_lambda': 3.3416874863644837, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029653 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028184 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "Training until validation scores don't improve for 20 rounds                    \n",
      "Early stopping, best iteration is:                                              \n",
      "[139]\tcv_agg's rmse: 3.69032 + 0.00588727\n",
      "{'bagging_fraction': 0.8513912813002616, 'bagging_freq': 9, 'feature_fraction': 0.5112561605454179, 'learning_rate': 0.16063924284732162, 'min_child_samples': 39, 'num_leaves': 240, 'reg_alpha': 5, 'reg_lambda': 2.2873092432384636, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024220 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022264 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "Training until validation scores don't improve for 20 rounds                    \n",
      "Early stopping, best iteration is:                                              \n",
      "[10]\tcv_agg's rmse: 3.71925 + 0.00374216\n",
      "{'bagging_fraction': 0.9270416910660276, 'bagging_freq': 8, 'feature_fraction': 0.5899018017071412, 'learning_rate': 0.2086422969309052, 'min_child_samples': 21, 'num_leaves': 140, 'reg_alpha': 1, 'reg_lambda': 4.87837731863211, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020898 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021253 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "Training until validation scores don't improve for 20 rounds                    \n",
      "Early stopping, best iteration is:                                              \n",
      "[8]\tcv_agg's rmse: 3.74176 + 0.0128902\n",
      "{'bagging_fraction': 0.8029777683473334, 'bagging_freq': 2, 'feature_fraction': 0.7436496671325717, 'learning_rate': 0.04673519425484282, 'min_child_samples': 30, 'num_leaves': 150, 'reg_alpha': 3, 'reg_lambda': 1.2274019030878773, 'feature_pre_filter': False, 'objective': 'regression', 'metric': 'rmse', 'bagging_seed': 2020}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066524 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066196 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 66433                                              \n",
      "[LightGBM] [Info] Number of data points in the train set: 100958, number of used features: 300\n",
      "Training until validation scores don't improve for 20 rounds                    \n",
      "Early stopping, best iteration is:                                              \n",
      "[62]\tcv_agg's rmse: 3.70199 + 0.0104611\n",
      "100%|█████████| 30/30 [05:41<00:00, 11.39s/trial, best loss: 3.6847801524169728]\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029340 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 65990\n",
      "[LightGBM] [Info] Number of data points in the train set: 161533, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.390986\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\ttrain's rmse: 3.70045\tvalid's rmse: 3.7555\n",
      "[100]\ttrain's rmse: 3.61903\tvalid's rmse: 3.71931\n",
      "[150]\ttrain's rmse: 3.56151\tvalid's rmse: 3.70468\n",
      "[200]\ttrain's rmse: 3.52009\tvalid's rmse: 3.69467\n",
      "[250]\ttrain's rmse: 3.48525\tvalid's rmse: 3.68996\n",
      "[300]\ttrain's rmse: 3.45373\tvalid's rmse: 3.68605\n",
      "[350]\ttrain's rmse: 3.42507\tvalid's rmse: 3.68383\n",
      "[400]\ttrain's rmse: 3.39806\tvalid's rmse: 3.68157\n",
      "[450]\ttrain's rmse: 3.37277\tvalid's rmse: 3.68026\n",
      "[500]\ttrain's rmse: 3.34913\tvalid's rmse: 3.67878\n",
      "[550]\ttrain's rmse: 3.32746\tvalid's rmse: 3.67791\n",
      "[600]\ttrain's rmse: 3.3073\tvalid's rmse: 3.67728\n",
      "[650]\ttrain's rmse: 3.28665\tvalid's rmse: 3.67625\n",
      "[700]\ttrain's rmse: 3.26698\tvalid's rmse: 3.67523\n",
      "[750]\ttrain's rmse: 3.24873\tvalid's rmse: 3.6748\n",
      "[800]\ttrain's rmse: 3.22834\tvalid's rmse: 3.67373\n",
      "[850]\ttrain's rmse: 3.20971\tvalid's rmse: 3.67295\n",
      "[900]\ttrain's rmse: 3.19193\tvalid's rmse: 3.67257\n",
      "[950]\ttrain's rmse: 3.17487\tvalid's rmse: 3.67205\n",
      "[1000]\ttrain's rmse: 3.15811\tvalid's rmse: 3.67207\n",
      "Early stopping, best iteration is:\n",
      "[990]\ttrain's rmse: 3.16118\tvalid's rmse: 3.67196\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026760 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66004\n",
      "[LightGBM] [Info] Number of data points in the train set: 161533, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396781\n",
      "[50]\ttrain's rmse: 3.70941\tvalid's rmse: 3.72398\n",
      "[100]\ttrain's rmse: 3.62839\tvalid's rmse: 3.68566\n",
      "[150]\ttrain's rmse: 3.57247\tvalid's rmse: 3.66835\n",
      "[200]\ttrain's rmse: 3.53012\tvalid's rmse: 3.66018\n",
      "[250]\ttrain's rmse: 3.49657\tvalid's rmse: 3.65481\n",
      "[300]\ttrain's rmse: 3.46602\tvalid's rmse: 3.65119\n",
      "[350]\ttrain's rmse: 3.43868\tvalid's rmse: 3.64738\n",
      "[400]\ttrain's rmse: 3.41088\tvalid's rmse: 3.64563\n",
      "[450]\ttrain's rmse: 3.38744\tvalid's rmse: 3.6438\n",
      "[500]\ttrain's rmse: 3.36339\tvalid's rmse: 3.64154\n",
      "[550]\ttrain's rmse: 3.34119\tvalid's rmse: 3.63979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[600]\ttrain's rmse: 3.31938\tvalid's rmse: 3.63885\n",
      "[650]\ttrain's rmse: 3.29958\tvalid's rmse: 3.6383\n",
      "[700]\ttrain's rmse: 3.27816\tvalid's rmse: 3.63657\n",
      "[750]\ttrain's rmse: 3.25799\tvalid's rmse: 3.63608\n",
      "[800]\ttrain's rmse: 3.23897\tvalid's rmse: 3.63526\n",
      "[850]\ttrain's rmse: 3.22046\tvalid's rmse: 3.63504\n",
      "Early stopping, best iteration is:\n",
      "[847]\ttrain's rmse: 3.22142\tvalid's rmse: 3.635\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028622 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66017\n",
      "[LightGBM] [Info] Number of data points in the train set: 161534, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.390348\n",
      "[50]\ttrain's rmse: 3.69367\tvalid's rmse: 3.77574\n",
      "[100]\ttrain's rmse: 3.61224\tvalid's rmse: 3.74338\n",
      "[150]\ttrain's rmse: 3.55484\tvalid's rmse: 3.72881\n",
      "[200]\ttrain's rmse: 3.51168\tvalid's rmse: 3.71968\n",
      "[250]\ttrain's rmse: 3.47679\tvalid's rmse: 3.71401\n",
      "[300]\ttrain's rmse: 3.44687\tvalid's rmse: 3.70931\n",
      "[350]\ttrain's rmse: 3.42043\tvalid's rmse: 3.70641\n",
      "[400]\ttrain's rmse: 3.39535\tvalid's rmse: 3.70408\n",
      "[450]\ttrain's rmse: 3.37111\tvalid's rmse: 3.70256\n",
      "[500]\ttrain's rmse: 3.34806\tvalid's rmse: 3.70139\n",
      "[550]\ttrain's rmse: 3.32507\tvalid's rmse: 3.7003\n",
      "[600]\ttrain's rmse: 3.3044\tvalid's rmse: 3.6995\n",
      "[650]\ttrain's rmse: 3.28322\tvalid's rmse: 3.69859\n",
      "[700]\ttrain's rmse: 3.26353\tvalid's rmse: 3.69816\n",
      "[750]\ttrain's rmse: 3.24503\tvalid's rmse: 3.69723\n",
      "[800]\ttrain's rmse: 3.22675\tvalid's rmse: 3.69705\n",
      "[850]\ttrain's rmse: 3.20838\tvalid's rmse: 3.69654\n",
      "Early stopping, best iteration is:\n",
      "[847]\ttrain's rmse: 3.22142\tvalid's rmse: 3.635\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028192 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66003\n",
      "[LightGBM] [Info] Number of data points in the train set: 161534, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.391392\n",
      "[50]\ttrain's rmse: 3.67556\tvalid's rmse: 3.8544\n",
      "[100]\ttrain's rmse: 3.59417\tvalid's rmse: 3.81821\n",
      "[150]\ttrain's rmse: 3.53747\tvalid's rmse: 3.80308\n",
      "[200]\ttrain's rmse: 3.49711\tvalid's rmse: 3.79538\n",
      "[250]\ttrain's rmse: 3.46065\tvalid's rmse: 3.7897\n",
      "[300]\ttrain's rmse: 3.42968\tvalid's rmse: 3.78595\n",
      "[350]\ttrain's rmse: 3.40208\tvalid's rmse: 3.78326\n",
      "[400]\ttrain's rmse: 3.37719\tvalid's rmse: 3.78126\n",
      "[450]\ttrain's rmse: 3.35309\tvalid's rmse: 3.77932\n",
      "[500]\ttrain's rmse: 3.32887\tvalid's rmse: 3.77739\n",
      "[550]\ttrain's rmse: 3.30565\tvalid's rmse: 3.77635\n",
      "[600]\ttrain's rmse: 3.28374\tvalid's rmse: 3.7746\n",
      "[650]\ttrain's rmse: 3.2639\tvalid's rmse: 3.77379\n",
      "[700]\ttrain's rmse: 3.24305\tvalid's rmse: 3.77289\n",
      "[750]\ttrain's rmse: 3.22403\tvalid's rmse: 3.77199\n",
      "[800]\ttrain's rmse: 3.20592\tvalid's rmse: 3.77085\n",
      "[850]\ttrain's rmse: 3.18686\tvalid's rmse: 3.77019\n",
      "Early stopping, best iteration is:\n",
      "[847]\ttrain's rmse: 3.22142\tvalid's rmse: 3.635\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029074 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66016\n",
      "[LightGBM] [Info] Number of data points in the train set: 161534, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.398675\n",
      "[50]\ttrain's rmse: 3.72701\tvalid's rmse: 3.64248\n",
      "[100]\ttrain's rmse: 3.64513\tvalid's rmse: 3.61033\n",
      "[150]\ttrain's rmse: 3.58956\tvalid's rmse: 3.59679\n",
      "[200]\ttrain's rmse: 3.54599\tvalid's rmse: 3.58856\n",
      "[250]\ttrain's rmse: 3.5096\tvalid's rmse: 3.58365\n",
      "[300]\ttrain's rmse: 3.47855\tvalid's rmse: 3.58017\n",
      "[350]\ttrain's rmse: 3.4523\tvalid's rmse: 3.57771\n",
      "[400]\ttrain's rmse: 3.42629\tvalid's rmse: 3.57513\n",
      "[450]\ttrain's rmse: 3.40095\tvalid's rmse: 3.57354\n",
      "[500]\ttrain's rmse: 3.37709\tvalid's rmse: 3.57279\n",
      "[550]\ttrain's rmse: 3.35495\tvalid's rmse: 3.57112\n",
      "[600]\ttrain's rmse: 3.33477\tvalid's rmse: 3.57014\n",
      "[650]\ttrain's rmse: 3.315\tvalid's rmse: 3.56934\n",
      "[700]\ttrain's rmse: 3.29549\tvalid's rmse: 3.56862\n",
      "[750]\ttrain's rmse: 3.27677\tvalid's rmse: 3.56742\n",
      "[800]\ttrain's rmse: 3.25903\tvalid's rmse: 3.56646\n",
      "[850]\ttrain's rmse: 3.24055\tvalid's rmse: 3.56601\n",
      "[900]\ttrain's rmse: 3.22274\tvalid's rmse: 3.56541\n",
      "Early stopping, best iteration is:\n",
      "[906]\ttrain's rmse: 3.22048\tvalid's rmse: 3.56536\n",
      "[3.671956482832931, 3.6350015892345326, 3.6966732201184125, 3.7704173016613085, 3.5653634108368886] 3.667882400936814\n"
     ]
    }
   ],
   "source": [
    "def train_predict(train, test, params):\n",
    "    \"\"\"\n",
    "\n",
    "    :param train:\n",
    "    :param test:\n",
    "    :param params:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    label = 'target'\n",
    "    features = train.columns.tolist()\n",
    "    features.remove('card_id')\n",
    "    features.remove('target')\n",
    "    params = params_append(params)\n",
    "    kf = KFold(n_splits=5, random_state=2020, shuffle=True)\n",
    "    prediction_test = 0\n",
    "    cv_score = []\n",
    "    prediction_train = pd.Series(index=train.index)\n",
    "    ESR = 30\n",
    "    NBR = 10000\n",
    "    VBE = 50\n",
    "    callbacks = [lgb.log_evaluation(VBE),lgb.early_stopping(ESR)]\n",
    "    for i,(train_part_index, val_index) in enumerate(kf.split(train[features], train[label])):\n",
    "        # 模型训练\n",
    "        train_part = lgb.Dataset(train[features].loc[train_part_index],\n",
    "                                 train[label].loc[train_part_index])\n",
    "        val_part = lgb.Dataset(train[features].loc[val_index],\n",
    "                           train[label].loc[val_index])\n",
    "        #这里的bst是boost不是best\n",
    "        bst = lgb.train(params, train_part, num_boost_round=NBR,\n",
    "                        valid_sets=[train_part, val_part],\n",
    "                        valid_names=['train', 'valid'],\n",
    "                        callbacks=callbacks)\n",
    "        #k-fold的每一次都用来预测一下test\n",
    "        prediction_test += bst.predict(test[features])\n",
    "        \n",
    "        #k-fold每一次都预测不同的val，拼在一次后相当于是预测了完整的train，实现了k-fold交叉验证法的样本内误差计算准备\n",
    "        prediction_train = pd.concat([prediction_train,pd.Series(bst.predict(train[features].loc[val_index]),\n",
    "                                                             index=val_index)],ignore_index=False)\n",
    "\n",
    "        #计算每次对val预测得到的分数\n",
    "        val_pre = bst.predict(train[features].loc[val_index])\n",
    "        score = np.sqrt(mean_squared_error(train[label].loc[val_index].values, val_pre))\n",
    "        cv_score.append(score)\n",
    "    print(cv_score, sum(cv_score) / 5)\n",
    "    pd.Series(prediction_train.sort_index().values).to_csv(path+\"preprocess/train_lightgbm.csv\", index=False)\n",
    "    pd.Series(prediction_test / 5).to_csv(path+\"preprocess/test_lightgbm.csv\", index=False)\n",
    "    test['target'] = prediction_test / 5\n",
    "    test[['card_id', 'target']].to_csv(path+\"result/submission_lightgbm.csv\", index=False)\n",
    "    return\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train, test = read_data(debug=False)\n",
    "    train, test = feature_select_wrapper(train, test)\n",
    "    best_clf = param_hyperopt(train)\n",
    "    train_predict(train, test, best_clf)\n",
    "# [3.686192535745703, 3.647032390847285, 3.706089838227353, 3.773664215095074, 3.5735473296458626] 3.677305261912256\n",
    "# [3.671956482832931, 3.635001589234533, 3.696673220118413, 3.770417301661309, 3.5653634108368886] 3.667882400936814"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c250c413",
   "metadata": {},
   "source": [
    "# 对比不采用贝叶斯调参的线下CV结果 3.6783\n",
    "(相比baseline-0.0074，相比贝叶斯调参+0.0104）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2cbdc4e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T12:57:02.160765Z",
     "start_time": "2024-04-25T12:53:02.327501Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_data...\n",
      "done\n",
      "feature_select_wrapper...\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.421605 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 227965\n",
      "[LightGBM] [Info] Number of data points in the train set: 161533, number of used features: 1637\n",
      "[LightGBM] [Info] Start training from score -0.390986\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\ttrain's rmse: 3.43102\tvalid's rmse: 3.70777\n",
      "[100]\ttrain's rmse: 3.29549\tvalid's rmse: 3.70918\n",
      "[150]\ttrain's rmse: 3.2017\tvalid's rmse: 3.71565\n",
      "Early stopping, best iteration is:\n",
      "[57]\ttrain's rmse: 3.41246\tvalid's rmse: 3.70645\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.415079 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 228067\n",
      "[LightGBM] [Info] Number of data points in the train set: 161533, number of used features: 1640\n",
      "[LightGBM] [Info] Start training from score -0.396781\n",
      "[50]\ttrain's rmse: 3.44484\tvalid's rmse: 3.66146\n",
      "[100]\ttrain's rmse: 3.31554\tvalid's rmse: 3.66409\n",
      "[150]\ttrain's rmse: 3.21636\tvalid's rmse: 3.66967\n",
      "Early stopping, best iteration is:\n",
      "[66]\ttrain's rmse: 3.39923\tvalid's rmse: 3.65861\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.197593 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 228163\n",
      "[LightGBM] [Info] Number of data points in the train set: 161534, number of used features: 1644\n",
      "[LightGBM] [Info] Start training from score -0.390348\n",
      "[50]\ttrain's rmse: 3.42185\tvalid's rmse: 3.73025\n",
      "[100]\ttrain's rmse: 3.29317\tvalid's rmse: 3.73649\n",
      "[150]\ttrain's rmse: 3.18987\tvalid's rmse: 3.7425\n",
      "Early stopping, best iteration is:\n",
      "[66]\ttrain's rmse: 3.39923\tvalid's rmse: 3.65861\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.208035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 228023\n",
      "[LightGBM] [Info] Number of data points in the train set: 161534, number of used features: 1639\n",
      "[LightGBM] [Info] Start training from score -0.391392\n",
      "[50]\ttrain's rmse: 3.4108\tvalid's rmse: 3.78929\n",
      "[100]\ttrain's rmse: 3.28691\tvalid's rmse: 3.79012\n",
      "[150]\ttrain's rmse: 3.17443\tvalid's rmse: 3.7956\n",
      "Early stopping, best iteration is:\n",
      "[66]\ttrain's rmse: 3.39923\tvalid's rmse: 3.65861\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.466016 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 227966\n",
      "[LightGBM] [Info] Number of data points in the train set: 161534, number of used features: 1637\n",
      "[LightGBM] [Info] Start training from score -0.398675\n",
      "[50]\ttrain's rmse: 3.46176\tvalid's rmse: 3.58679\n",
      "[100]\ttrain's rmse: 3.33139\tvalid's rmse: 3.5886\n",
      "[150]\ttrain's rmse: 3.23089\tvalid's rmse: 3.59955\n",
      "Early stopping, best iteration is:\n",
      "[50]\ttrain's rmse: 3.46176\tvalid's rmse: 3.58679\n",
      "done\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.086457 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 65990\n",
      "[LightGBM] [Info] Number of data points in the train set: 161533, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.390986\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\ttrain's rmse: 3.52536\tvalid's rmse: 3.69919\n",
      "[100]\ttrain's rmse: 3.42447\tvalid's rmse: 3.6871\n",
      "[150]\ttrain's rmse: 3.35333\tvalid's rmse: 3.6852\n",
      "[200]\ttrain's rmse: 3.28939\tvalid's rmse: 3.6843\n",
      "[250]\ttrain's rmse: 3.22892\tvalid's rmse: 3.68425\n",
      "[300]\ttrain's rmse: 3.17659\tvalid's rmse: 3.68423\n",
      "[350]\ttrain's rmse: 3.12768\tvalid's rmse: 3.68446\n",
      "[400]\ttrain's rmse: 3.082\tvalid's rmse: 3.68574\n",
      "Early stopping, best iteration is:\n",
      "[327]\ttrain's rmse: 3.15022\tvalid's rmse: 3.68356\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.086296 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 66004\n",
      "[LightGBM] [Info] Number of data points in the train set: 161533, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.396781\n",
      "[50]\ttrain's rmse: 3.54103\tvalid's rmse: 3.66676\n",
      "[100]\ttrain's rmse: 3.43878\tvalid's rmse: 3.6557\n",
      "[150]\ttrain's rmse: 3.36506\tvalid's rmse: 3.65199\n",
      "[200]\ttrain's rmse: 3.29667\tvalid's rmse: 3.65078\n",
      "[250]\ttrain's rmse: 3.23905\tvalid's rmse: 3.65017\n",
      "[300]\ttrain's rmse: 3.18893\tvalid's rmse: 3.6504\n",
      "[350]\ttrain's rmse: 3.1375\tvalid's rmse: 3.65263\n",
      "Early stopping, best iteration is:\n",
      "[288]\ttrain's rmse: 3.20208\tvalid's rmse: 3.64924\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.089957 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 66017\n",
      "[LightGBM] [Info] Number of data points in the train set: 161534, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.390348\n",
      "[50]\ttrain's rmse: 3.51974\tvalid's rmse: 3.72145\n",
      "[100]\ttrain's rmse: 3.41602\tvalid's rmse: 3.70582\n",
      "[150]\ttrain's rmse: 3.34184\tvalid's rmse: 3.70517\n",
      "[200]\ttrain's rmse: 3.27842\tvalid's rmse: 3.70341\n",
      "[250]\ttrain's rmse: 3.22486\tvalid's rmse: 3.70401\n",
      "[300]\ttrain's rmse: 3.16858\tvalid's rmse: 3.7052\n",
      "[350]\ttrain's rmse: 3.12035\tvalid's rmse: 3.70556\n",
      "Early stopping, best iteration is:\n",
      "[288]\ttrain's rmse: 3.20208\tvalid's rmse: 3.64924\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042703 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 66003\n",
      "[LightGBM] [Info] Number of data points in the train set: 161534, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.391392\n",
      "[50]\ttrain's rmse: 3.50557\tvalid's rmse: 3.79471\n",
      "[100]\ttrain's rmse: 3.40216\tvalid's rmse: 3.78383\n",
      "[150]\ttrain's rmse: 3.32686\tvalid's rmse: 3.77923\n",
      "[200]\ttrain's rmse: 3.26683\tvalid's rmse: 3.77794\n",
      "[250]\ttrain's rmse: 3.20905\tvalid's rmse: 3.77729\n",
      "[300]\ttrain's rmse: 3.16\tvalid's rmse: 3.77963\n",
      "[350]\ttrain's rmse: 3.11235\tvalid's rmse: 3.77959\n",
      "Early stopping, best iteration is:\n",
      "[288]\ttrain's rmse: 3.20208\tvalid's rmse: 3.64924\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.090137 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 66016\n",
      "[LightGBM] [Info] Number of data points in the train set: 161534, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -0.398675\n",
      "[50]\ttrain's rmse: 3.55561\tvalid's rmse: 3.5938\n",
      "[100]\ttrain's rmse: 3.44989\tvalid's rmse: 3.58292\n",
      "[150]\ttrain's rmse: 3.38027\tvalid's rmse: 3.57869\n",
      "[200]\ttrain's rmse: 3.3196\tvalid's rmse: 3.5767\n",
      "[250]\ttrain's rmse: 3.26428\tvalid's rmse: 3.57649\n",
      "[300]\ttrain's rmse: 3.21109\tvalid's rmse: 3.57708\n",
      "[350]\ttrain's rmse: 3.16042\tvalid's rmse: 3.57864\n",
      "Early stopping, best iteration is:\n",
      "[264]\ttrain's rmse: 3.24906\tvalid's rmse: 3.57557\n",
      "[3.6835571959638775, 3.649239624220389, 3.7045643917821964, 3.7793488000045983, 3.5755689945682643] 3.678455801307865\n"
     ]
    }
   ],
   "source": [
    "def train_predict1(train, test, params):\n",
    "    \"\"\"\n",
    "\n",
    "    :param train:\n",
    "    :param test:\n",
    "    :param params:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    label = 'target'\n",
    "    features = train.columns.tolist()\n",
    "    features.remove('card_id')\n",
    "    features.remove('target')\n",
    "    params = params_append(params)\n",
    "    kf = KFold(n_splits=5, random_state=2020, shuffle=True)\n",
    "    prediction_test = 0\n",
    "    cv_score = []\n",
    "    prediction_train = pd.Series(index=train.index)\n",
    "    ESR = 30\n",
    "    NBR = 10000\n",
    "    VBE = 50\n",
    "    callbacks = [lgb.log_evaluation(VBE),lgb.early_stopping(ESR)]\n",
    "    for i,(train_part_index, val_index) in enumerate(kf.split(train[features], train[label])):\n",
    "        # 模型训练\n",
    "        train_part = lgb.Dataset(train[features].loc[train_part_index],\n",
    "                                 train[label].loc[train_part_index])\n",
    "        val_part = lgb.Dataset(train[features].loc[val_index],\n",
    "                           train[label].loc[val_index])\n",
    "        #这里的bst是boost不是best\n",
    "        bst = lgb.train(params, train_part, num_boost_round=NBR,\n",
    "                        valid_sets=[train_part, val_part],\n",
    "                        valid_names=['train', 'valid'],\n",
    "                        callbacks=callbacks)\n",
    "        #k-fold的每一次都用来预测一下test\n",
    "        prediction_test += bst.predict(test[features])\n",
    "        \n",
    "        #k-fold每一次都预测不同的val，拼在一次后相当于是预测了完整的train，实现了k-fold交叉验证法的样本内误差计算准备\n",
    "        prediction_train = pd.concat([prediction_train,pd.Series(bst.predict(train[features].loc[val_index]),\n",
    "                                                             index=val_index)],ignore_index=False)\n",
    "\n",
    "        #计算每次对val预测得到的分数\n",
    "        val_pre = bst.predict(train[features].loc[val_index])\n",
    "        score = np.sqrt(mean_squared_error(train[label].loc[val_index].values, val_pre))\n",
    "        cv_score.append(score)\n",
    "    print(cv_score, sum(cv_score) / 5)\n",
    "    return\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train, test = read_data(debug=False)\n",
    "    train, test = feature_select_wrapper(train, test)\n",
    "    params_initial = {\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'boosting': 'gbdt',\n",
    "        'min_child_samples': 20,\n",
    "        'bagging_seed': 2020,\n",
    "        'bagging_fraction': 0.7, #行采样\n",
    "        'bagging_freq': 1,#重新采样的频率，每k次迭代后重新按照bagging_fraction比例进行行采样\n",
    "        'feature_fraction': 0.7, #列采样\n",
    "        'max_depth': -1,\n",
    "        'metric': 'rmse',\n",
    "        'reg_alpha': 0,\n",
    "        'reg_lambda': 1,\n",
    "        'objective': 'regression'\n",
    "    }\n",
    "    train_predict1(train, test, params_initial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc25ec21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
