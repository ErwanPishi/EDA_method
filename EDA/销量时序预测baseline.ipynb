{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de7a87e3-d759-4c72-951b-af650fb36489",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% 赛题目的：预测单位销量(unit_sales) 书上第11章\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import swifter\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from datetime import date,datetime,timedelta    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d32a419b-9384-45e7-9d5b-239f743d7ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 113.05473113059998\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91602352179347cdb53effd061d2fdc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dask Apply:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ziyan/opt/anaconda3/lib/python3.9/site-packages/dask/utils.py:1241: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  return getattr(__obj, self.method)(*args, **kwargs)\n",
      "/Users/ziyan/opt/anaconda3/lib/python3.9/site-packages/dask/utils.py:1241: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  return getattr(__obj, self.method)(*args, **kwargs)\n",
      "/Users/ziyan/opt/anaconda3/lib/python3.9/site-packages/dask/utils.py:1241: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  return getattr(__obj, self.method)(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: 58.740147829055786\n"
     ]
    }
   ],
   "source": [
    "#%% 数据读取\n",
    "path = '/Users/ziyan/Documents/机器学习/机器学习比赛/favorita-grocery-sales-forecasting/'\n",
    "# 只用pandas\n",
    "time1 = time.time()\n",
    "df_train = pd.read_csv(path+'train.csv',parse_dates=['date'])\n",
    "df_train['unit_sales'] = df_train['unit_sales'].astype(float).apply(lambda u:np.log1p(u) if u>0 else 0)\n",
    "print('1:',time.time() - time1)\n",
    "\n",
    "# 用polars读再转swifter ，节省一半时间 注意这部分代码可能要单独运行 有几率失败\n",
    "time2 = time.time()\n",
    "df_train = pl.read_csv(path+'train.csv',dtypes={'date': pl.Datetime,'onpromotion':bool,'unit_sales':float})\n",
    "df_train = df_train.to_pandas()\n",
    "df_train['unit_sales'] = df_train['unit_sales'].swifter.apply(lambda u:np.log1p(u) if u>0 else 0.0)\n",
    "print('2:',time.time() - time2)\n",
    "\n",
    "df_test = pd.read_csv(path+'test.csv',parse_dates=['date'])\n",
    "\n",
    "items = pd.read_csv(path+'items.csv')\n",
    "stores = pd.read_csv(path+'stores.csv')\n",
    "\n",
    "df_train.isnull().sum() \n",
    "# onpromotion有21657651个缺失\n",
    "df_train['onpromotion'].value_counts()\n",
    "# False 96028767,True 7810622\n",
    "z = df_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f299f32-8248-43b6-b0aa-262396339785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.370464e+06</td>\n",
       "      <td>3370464</td>\n",
       "      <td>3.370464e+06</td>\n",
       "      <td>3.370464e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.271823e+08</td>\n",
       "      <td>2017-08-23 12:00:00.000003072</td>\n",
       "      <td>2.750000e+01</td>\n",
       "      <td>1.244798e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.254970e+08</td>\n",
       "      <td>2017-08-16 00:00:00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>9.699500e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.263397e+08</td>\n",
       "      <td>2017-08-19 18:00:00</td>\n",
       "      <td>1.400000e+01</td>\n",
       "      <td>8.053210e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.271823e+08</td>\n",
       "      <td>2017-08-23 12:00:00</td>\n",
       "      <td>2.750000e+01</td>\n",
       "      <td>1.294665e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.280249e+08</td>\n",
       "      <td>2017-08-27 06:00:00</td>\n",
       "      <td>4.100000e+01</td>\n",
       "      <td>1.730015e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.288675e+08</td>\n",
       "      <td>2017-08-31 00:00:00</td>\n",
       "      <td>5.400000e+01</td>\n",
       "      <td>2.134244e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.729693e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.558579e+01</td>\n",
       "      <td>5.898362e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                           date     store_nbr      item_nbr\n",
       "count  3.370464e+06                        3370464  3.370464e+06  3.370464e+06\n",
       "mean   1.271823e+08  2017-08-23 12:00:00.000003072  2.750000e+01  1.244798e+06\n",
       "min    1.254970e+08            2017-08-16 00:00:00  1.000000e+00  9.699500e+04\n",
       "25%    1.263397e+08            2017-08-19 18:00:00  1.400000e+01  8.053210e+05\n",
       "50%    1.271823e+08            2017-08-23 12:00:00  2.750000e+01  1.294665e+06\n",
       "75%    1.280249e+08            2017-08-27 06:00:00  4.100000e+01  1.730015e+06\n",
       "max    1.288675e+08            2017-08-31 00:00:00  5.400000e+01  2.134244e+06\n",
       "std    9.729693e+05                            NaN  1.558579e+01  5.898362e+05"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0e06e2d-5fc7-4822-9737-128ce0002feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id                        date     store_nbr      item_nbr  \\\n",
      "count  2.380826e+07                    23808261  2.380826e+07  2.380826e+07   \n",
      "mean   1.135929e+08  2017-04-24 15:38:17.344120  2.794909e+01  1.160406e+06   \n",
      "min    1.016888e+08         2017-01-01 00:00:00  1.000000e+00  9.699500e+04   \n",
      "25%    1.076408e+08         2017-02-27 00:00:00  1.300000e+01  6.875490e+05   \n",
      "50%    1.135929e+08         2017-04-25 00:00:00  2.800000e+01  1.176562e+06   \n",
      "75%    1.195450e+08         2017-06-20 00:00:00  4.300000e+01  1.501544e+06   \n",
      "max    1.254970e+08         2017-08-15 00:00:00  5.400000e+01  2.127114e+06   \n",
      "std    6.872853e+06                         NaN  1.621998e+01  5.795109e+05   \n",
      "\n",
      "         unit_sales  \n",
      "count  2.380826e+07  \n",
      "mean   1.698823e+00  \n",
      "min    0.000000e+00  \n",
      "25%    1.098612e+00  \n",
      "50%    1.609438e+00  \n",
      "75%    2.197225e+00  \n",
      "max    9.749579e+00  \n",
      "std    8.723154e-01  \n",
      " \n",
      "id             0\n",
      "date           0\n",
      "store_nbr      0\n",
      "item_nbr       0\n",
      "unit_sales     0\n",
      "onpromotion    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#%% 数据准备 \n",
    "# 只保留2017年的数据，利用久远的数据对未来预测会产生一定的噪声（两者不太相关）\n",
    "df_2017 = df_train[df_train.date >= pd.Timestamp(2017,1,1)]\n",
    "del df_train\n",
    "\n",
    "print(df_2017.describe())\n",
    "print(' ')\n",
    "print(df_2017.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "852f7eb7-7fc7-4093-ac1a-e790c15808e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% 特征：train中出现的store_nbr✖️item_nbr的商品是否promo（test中新出现的不管\n",
    "# 先转化为三维索引，再按照日期做unstack，每一列都是不同的日期 第一列2017-1-1，第二列2017-1-2，\n",
    "# unstack(level=-1)把最后一维（日期）作为列 fillna(False) 把空值填为False\n",
    "# 这样就处理成了按列走向的时间序列 尚不明确意义\n",
    "promo_2017_train = df_2017.set_index(['store_nbr','item_nbr','date'])[['onpromotion']].unstack(level=-1).fillna(False)\n",
    "#help(pd.MultiIndex.get_level_values)\n",
    "promo_2017_train.columns = promo_2017_train.columns.get_level_values(1)\n",
    "\n",
    "promo_2017_test = df_test.set_index(['store_nbr','item_nbr','date'])[['onpromotion']].unstack(level=-1).fillna(False)\n",
    "promo_2017_test.columns = promo_2017_test.columns.get_level_values(1)\n",
    "promo_2017_test = promo_2017_test.reindex(promo_2017_train.index).fillna(False)\n",
    "# 强行把索引搞成一样，类似于用promo_2017_train来LEFTJOIN test（因为这是找train?）\n",
    "promo_2017 = pd.concat([promo_2017_train,promo_2017_test],axis=1)\n",
    "# axis=1 列取并集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdfcef7f-7f46-40af-9cb7-a5fc59a5a3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% 标签：unit_sales\n",
    "df_2017 = df_2017.set_index(['store_nbr','item_nbr','date'])[['unit_sales']].unstack(level=-1).fillna(0)\n",
    "# fillna(0)是从实际出发，那一天没有记录就代表没有卖出东西\n",
    "df_2017.columns = df_2017.columns.get_level_values(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "280d892a-347a-439f-91b7-61981c7e6376",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% 特征提取\n",
    "def get_date_range(df,dt,forward_steps,periods,freq='D'):\n",
    "    '''\n",
    "    Description\n",
    "    ----------\n",
    "    截取时间段的通用函数\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        按照日期在列的方式处理好的df.\n",
    "    dt : datetime.date\n",
    "        某一天，例如date(2017,2,28).\n",
    "    forward_steps : int\n",
    "        回溯的天数，注意这里的steps指天数.\n",
    "    periods : int\n",
    "        整个date_range一共含多少个日期.\n",
    "    freq : str, optional\n",
    "        步长，'D'表示1天，'7D'表示7天. The default is 'D'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        返回df对应日期序列的记录.\n",
    "\n",
    "    '''\n",
    "    return df[pd.date_range(start=dt-timedelta(days=forward_steps),\n",
    "                            periods=periods,#periods是总个数\n",
    "                            freq=freq)] #freq表示步长 \n",
    "\n",
    "def prepare_dataset(t2017,is_train=True):\n",
    "    X = pd.DataFrame({\n",
    "        # 历史平移特征： t-1，t-2，t-3天的销量\n",
    "        # 字典方式构造dataframe\n",
    "        'day_1_hist':get_date_range(df_2017,t2017,1,1).values.ravel(),#按照行拉伸至1维\n",
    "        'day_2_hist':get_date_range(df_2017,t2017,2,1).values.ravel(),#按照行拉伸至1维\n",
    "        'day_3_hist':get_date_range(df_2017,t2017,3,1).values.ravel(),#按照行拉伸至1维        \n",
    "    })\n",
    "\n",
    "    for i in [7,14,21,30]:\n",
    "        # 窗口统计特征，销量diff/mean/med/max/min/std\n",
    "        # 注意下面的批量命名变量方式 diff(axis=1)是某一行中第t+1列元素减去第t列元素 mean(axis=1)是某一行中对每一列元素求均值\n",
    "        # axis=1则最后行数不变 列数可能有变化\n",
    "        # 「i天销量之差」的均值\n",
    "        X['diff_{}_day_mean'.format(i)] = get_date_range(df_2017,t2017,i,i).diff(axis=1).mean(axis=1).values\n",
    "        # i天销量的均值\n",
    "        X['mean_{}_day'.format(i)] = get_date_range(df_2017,t2017,i,i).mean(axis=1).values\n",
    "        # i天销量的中位数\n",
    "        X['median_{}_day'.format(i)] = get_date_range(df_2017,t2017,i,i).median(axis=1).values\n",
    "        # i天销量的最大值\n",
    "        X['max_{}_day'.format(i)] = get_date_range(df_2017,t2017,i,i).max(axis=1).values\n",
    "        # i天销量的最小值\n",
    "        X['min_{}_day'.format(i)] = get_date_range(df_2017,t2017,i,i).min(axis=1).values\n",
    "        # i天销量的标准差\n",
    "        X['std_{}_day'.format(i)] = get_date_range(df_2017,t2017,i,i).std(axis=1).values\n",
    "\n",
    "    for i in range(7):\n",
    "        # 前4周（本周不算）每周第i+1天(i=0~6)的平均销量\n",
    "        X['mean_4_dow{}_2017'.format(i)] = get_date_range(df_2017,t2017,28-i,4,freq='7D').mean(axis=1).values\n",
    "        # 前10周（本周不算）每周第i+1天(i=0~6)的平均销量\n",
    "        X['mean_10_dow{}_2017'.format(i)] = get_date_range(df_2017,t2017,70-i,10,freq='7D').mean(axis=1).values\n",
    "\n",
    "    for i in range(16):\n",
    "        # 未来16天是否为促销日（为什么是16天？）\n",
    "        X['promo_{}'.format(i)] = promo_2017[str(t2017+timedelta(days=i))].values\n",
    "        \n",
    "    if is_train:\n",
    "    # 如果是训练数据，由于df数据的列是日期，选择未来16天作为y_train\n",
    "        y = df_2017[pd.date_range(t2017,periods=16)].values\n",
    "        return X,y\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "659d2ae6-c884-457d-be21-0a0379b225e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 14/14 [00:13<00:00,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "#%% 划分样本集\n",
    "X_l,y_l = [],[]\n",
    "t2017 = date(2017,7,5)\n",
    "n_range = 14\n",
    "for i in tqdm(range(n_range)):\n",
    "    delta = timedelta(days=7*i)\n",
    "    X_tmp,y_tmp = prepare_dataset(t2017-delta)\n",
    "    X_l.append(X_tmp)\n",
    "    y_l.append(y_tmp)\n",
    "    \n",
    "X_train = pd.concat(X_l,axis=0)\n",
    "y_train = np.concatenate(y_l,axis=0)\n",
    "del X_l,y_l\n",
    "    \n",
    "X_val,y_val = prepare_dataset(date(2017,7,26))\n",
    "X_test = prepare_dataset(date(2017,8,16),is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ba6dae5-d2cc-4152-bc52-163fd0a7787c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Step 1 ======\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.098092 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10487\n",
      "[LightGBM] [Info] Number of data points in the train set: 2345210, number of used features: 57\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score 1.018067\n",
      "[100]\ttraining's l2: 0.313668\tvalid_1's l2: 0.30201\n",
      "[200]\ttraining's l2: 0.309921\tvalid_1's l2: 0.299609\n",
      "[300]\ttraining's l2: 0.308087\tvalid_1's l2: 0.298714\n",
      "[400]\ttraining's l2: 0.306689\tvalid_1's l2: 0.298168\n",
      "[500]\ttraining's l2: 0.305649\tvalid_1's l2: 0.297879\n",
      "====== Step 2 ======\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.109009 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10487\n",
      "[LightGBM] [Info] Number of data points in the train set: 2345210, number of used features: 57\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score 0.950341\n",
      "[100]\ttraining's l2: 0.34022\tvalid_1's l2: 0.33226\n",
      "[200]\ttraining's l2: 0.336292\tvalid_1's l2: 0.329012\n",
      "[300]\ttraining's l2: 0.334486\tvalid_1's l2: 0.328163\n",
      "[400]\ttraining's l2: 0.333128\tvalid_1's l2: 0.327712\n",
      "[500]\ttraining's l2: 0.331946\tvalid_1's l2: 0.327345\n",
      "====== Step 3 ======\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114405 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10487\n",
      "[LightGBM] [Info] Number of data points in the train set: 2345210, number of used features: 57\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score 1.033715\n",
      "[100]\ttraining's l2: 0.351095\tvalid_1's l2: 0.348143\n",
      "[200]\ttraining's l2: 0.34616\tvalid_1's l2: 0.34493\n",
      "[300]\ttraining's l2: 0.344053\tvalid_1's l2: 0.343999\n",
      "[400]\ttraining's l2: 0.342572\tvalid_1's l2: 0.343771\n",
      "[500]\ttraining's l2: 0.341275\tvalid_1's l2: 0.343568\n",
      "====== Step 4 ======\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.106929 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10487\n",
      "[LightGBM] [Info] Number of data points in the train set: 2345210, number of used features: 57\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score 1.211011\n",
      "[100]\ttraining's l2: 0.367415\tvalid_1's l2: 0.360239\n",
      "[200]\ttraining's l2: 0.362286\tvalid_1's l2: 0.356315\n",
      "[300]\ttraining's l2: 0.360149\tvalid_1's l2: 0.355497\n",
      "[400]\ttraining's l2: 0.358548\tvalid_1's l2: 0.354949\n",
      "[500]\ttraining's l2: 0.357207\tvalid_1's l2: 0.354557\n",
      "====== Step 5 ======\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117422 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10487\n",
      "[LightGBM] [Info] Number of data points in the train set: 2345210, number of used features: 57\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score 1.235843\n",
      "[100]\ttraining's l2: 0.379045\tvalid_1's l2: 0.365538\n",
      "[200]\ttraining's l2: 0.373337\tvalid_1's l2: 0.361321\n",
      "[300]\ttraining's l2: 0.370977\tvalid_1's l2: 0.360053\n",
      "[400]\ttraining's l2: 0.36917\tvalid_1's l2: 0.359244\n",
      "[500]\ttraining's l2: 0.367678\tvalid_1's l2: 0.358796\n",
      "====== Step 6 ======\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117709 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10487\n",
      "[LightGBM] [Info] Number of data points in the train set: 2345210, number of used features: 57\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score 1.059264\n",
      "[100]\ttraining's l2: 0.37849\tvalid_1's l2: 0.36792\n",
      "[200]\ttraining's l2: 0.373952\tvalid_1's l2: 0.364252\n",
      "[300]\ttraining's l2: 0.371845\tvalid_1's l2: 0.363119\n",
      "[400]\ttraining's l2: 0.370314\tvalid_1's l2: 0.362536\n",
      "[500]\ttraining's l2: 0.369021\tvalid_1's l2: 0.362192\n",
      "====== Step 7 ======\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119121 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10487\n",
      "[LightGBM] [Info] Number of data points in the train set: 2345210, number of used features: 57\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score 1.011647\n",
      "[100]\ttraining's l2: 0.359951\tvalid_1's l2: 0.426474\n",
      "[200]\ttraining's l2: 0.35563\tvalid_1's l2: 0.423954\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[300]\ttraining's l2: 0.353634\tvalid_1's l2: 0.423317\n",
      "[400]\ttraining's l2: 0.352204\tvalid_1's l2: 0.423155\n",
      "[500]\ttraining's l2: 0.350995\tvalid_1's l2: 0.422905\n",
      "====== Step 8 ======\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118575 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10487\n",
      "[LightGBM] [Info] Number of data points in the train set: 2345210, number of used features: 57\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score 1.013161\n",
      "[100]\ttraining's l2: 0.351783\tvalid_1's l2: 0.39548\n",
      "[200]\ttraining's l2: 0.34766\tvalid_1's l2: 0.392201\n",
      "[300]\ttraining's l2: 0.345721\tvalid_1's l2: 0.391316\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[400]\ttraining's l2: 0.344284\tvalid_1's l2: 0.390825\n",
      "[500]\ttraining's l2: 0.343066\tvalid_1's l2: 0.390468\n",
      "====== Step 9 ======\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.126914 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10487\n",
      "[LightGBM] [Info] Number of data points in the train set: 2345210, number of used features: 57\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score 0.946614\n",
      "[100]\ttraining's l2: 0.367663\tvalid_1's l2: 0.382089\n",
      "[200]\ttraining's l2: 0.36345\tvalid_1's l2: 0.379847\n",
      "[300]\ttraining's l2: 0.361576\tvalid_1's l2: 0.379366\n",
      "[400]\ttraining's l2: 0.360086\tvalid_1's l2: 0.379136\n",
      "[500]\ttraining's l2: 0.358908\tvalid_1's l2: 0.378951\n",
      "====== Step 10 ======\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125252 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10487\n",
      "[LightGBM] [Info] Number of data points in the train set: 2345210, number of used features: 57\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score 1.033528\n",
      "[100]\ttraining's l2: 0.378328\tvalid_1's l2: 0.376551\n",
      "[200]\ttraining's l2: 0.373212\tvalid_1's l2: 0.373971\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[300]\ttraining's l2: 0.371002\tvalid_1's l2: 0.373263\n",
      "[400]\ttraining's l2: 0.369473\tvalid_1's l2: 0.372877\n",
      "[500]\ttraining's l2: 0.368173\tvalid_1's l2: 0.372702\n",
      "====== Step 11 ======\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10487\n",
      "[LightGBM] [Info] Number of data points in the train set: 2345210, number of used features: 57\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score 1.210464\n",
      "[100]\ttraining's l2: 0.400209\tvalid_1's l2: 0.38451\n",
      "[200]\ttraining's l2: 0.394843\tvalid_1's l2: 0.381632\n",
      "[300]\ttraining's l2: 0.392607\tvalid_1's l2: 0.380849\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[400]\ttraining's l2: 0.390953\tvalid_1's l2: 0.380437\n",
      "[500]\ttraining's l2: 0.389502\tvalid_1's l2: 0.38012\n",
      "====== Step 12 ======\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114585 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10487\n",
      "[LightGBM] [Info] Number of data points in the train set: 2345210, number of used features: 57\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score 1.236645\n",
      "[100]\ttraining's l2: 0.412428\tvalid_1's l2: 0.397086\n",
      "[200]\ttraining's l2: 0.40659\tvalid_1's l2: 0.3941\n",
      "[300]\ttraining's l2: 0.404001\tvalid_1's l2: 0.393158\n",
      "[400]\ttraining's l2: 0.40217\tvalid_1's l2: 0.39278\n",
      "[500]\ttraining's l2: 0.400701\tvalid_1's l2: 0.392537\n",
      "====== Step 13 ======\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.106124 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10487\n",
      "[LightGBM] [Info] Number of data points in the train set: 2345210, number of used features: 57\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score 1.063135\n",
      "[100]\ttraining's l2: 0.403686\tvalid_1's l2: 0.383462\n",
      "[200]\ttraining's l2: 0.398587\tvalid_1's l2: 0.381141\n",
      "[300]\ttraining's l2: 0.396352\tvalid_1's l2: 0.380619\n",
      "[400]\ttraining's l2: 0.394776\tvalid_1's l2: 0.380364\n",
      "[500]\ttraining's l2: 0.393435\tvalid_1's l2: 0.380203\n",
      "====== Step 14 ======\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116287 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10487\n",
      "[LightGBM] [Info] Number of data points in the train set: 2345210, number of used features: 57\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score 1.012088\n",
      "[100]\ttraining's l2: 0.37991\tvalid_1's l2: 0.368601\n",
      "[200]\ttraining's l2: 0.375259\tvalid_1's l2: 0.3666\n",
      "[300]\ttraining's l2: 0.373232\tvalid_1's l2: 0.366026\n",
      "[400]\ttraining's l2: 0.371643\tvalid_1's l2: 0.365806\n",
      "[500]\ttraining's l2: 0.370403\tvalid_1's l2: 0.365714\n",
      "====== Step 15 ======\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.113431 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10487\n",
      "[LightGBM] [Info] Number of data points in the train set: 2345210, number of used features: 57\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score 1.013815\n",
      "[100]\ttraining's l2: 0.370013\tvalid_1's l2: 0.3569\n",
      "[200]\ttraining's l2: 0.365629\tvalid_1's l2: 0.354913\n",
      "[300]\ttraining's l2: 0.36364\tvalid_1's l2: 0.35434\n",
      "[400]\ttraining's l2: 0.362219\tvalid_1's l2: 0.354092\n",
      "[500]\ttraining's l2: 0.361061\tvalid_1's l2: 0.353977\n",
      "====== Step 16 ======\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.107660 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10487\n",
      "[LightGBM] [Info] Number of data points in the train set: 2345210, number of used features: 57\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score 0.943426\n",
      "[100]\ttraining's l2: 0.383341\tvalid_1's l2: 0.378123\n",
      "[200]\ttraining's l2: 0.379169\tvalid_1's l2: 0.376261\n",
      "[300]\ttraining's l2: 0.377331\tvalid_1's l2: 0.375728\n",
      "[400]\ttraining's l2: 0.37599\tvalid_1's l2: 0.375539\n",
      "[500]\ttraining's l2: 0.37476\tvalid_1's l2: 0.375339\n"
     ]
    }
   ],
   "source": [
    "#%% 模型训练\n",
    "params = {\n",
    "    'num_leaves':2**5-1,\n",
    "    'objective':'regression_l2',\n",
    "    'max_depth':8,\n",
    "    'min_data_in_leaf':50,\n",
    "    'learning_rate':0.05,\n",
    "    'feature_fraction':0.75,#列采样\n",
    "    'bagging_fraction':0.75,#行采样\n",
    "    'bagging_freq':1,#每1侈迭代就bagging一次\n",
    "    'metric':'l2'\n",
    "}\n",
    "MAX_ROUNDS = 500\n",
    "val_pred=[]\n",
    "test_pred=[]\n",
    "\n",
    "#递归使用one-step-ahead，每次将预测值作为真实值加入训练集\n",
    "for i in range(16):\n",
    "    print('====== Step %d ======'%(i+1))\n",
    "    dtrain = lgb.Dataset(X_train,\n",
    "                         label=y_train[:,i])#一次选一天当标签\n",
    "    dval = lgb.Dataset(X_val,\n",
    "                       label=y_val[:,i],#一次选一天当标签\n",
    "                       reference=dtrain)#If this is Dataset for validation, training data should be used as reference.\n",
    "    bst = lgb.train(params,dtrain,MAX_ROUNDS,\n",
    "                    valid_sets=[dtrain,dval],verbose_eval=100)\n",
    "    \n",
    "    \n",
    "    val_pred.append(bst.predict(X_val,\n",
    "                               num_iteration=bst.best_iteration or MAX_ROUNDS))\n",
    "    test_pred.append(bst.predict(X_test,\n",
    "                                 num_iteration=bst.best_iteration or MAX_ROUNDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d3a307-90a4-440e-839b-8e0598204a76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
